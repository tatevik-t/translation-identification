{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "import transformers\n",
    "import onnxruntime as rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"XLM-RoBERTA - CLPD/10_langs-XLM-RoBERTa-base-100/XLM-RoBERTa-base.onnx\"\n",
    "\n",
    "MAX_LEN = 512\n",
    "fast_tokenizer = transformers.XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficiency(labels, predictions, th=0.5):\n",
    "    \"\"\"Returns accuracy, recall, precision, f1 according to actual labels and predictions\"\"\"\n",
    "    acc = tf.keras.metrics.BinaryAccuracy(threshold=th)\n",
    "    acc.update_state(labels, predictions[:, ])\n",
    "    acc_res = acc.result().numpy()\n",
    "    rec = tf.keras.metrics.Recall(thresholds=th)\n",
    "    rec.update_state(labels, predictions[:, ])\n",
    "    rec_res = rec.result().numpy()\n",
    "    prec = tf.keras.metrics.Precision(thresholds=th)\n",
    "    prec.update_state(labels, predictions[:, ])\n",
    "    prec_res = prec.result().numpy()\n",
    "\n",
    "    f1_res = 2 * ((rec_res*prec_res)/(rec_res+prec_res))\n",
    "\n",
    "    return acc_res, rec_res, prec_res, f1_res\n",
    "\n",
    "\n",
    "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n",
    "    \"\"\"Encodes texts using given tokenizer\"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "        chunk_ids = []\n",
    "        chunk_attention_mask = []\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        for s in text_chunk:\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                s,\n",
    "                add_special_tokens=True,\n",
    "                max_length=maxlen,\n",
    "                pad_to_max_length=True,\n",
    "                return_attention_mask=True\n",
    "            )\n",
    "            chunk_ids.append(encoded_dict['input_ids'])\n",
    "            chunk_attention_mask.append(encoded_dict['attention_mask'])\n",
    "        input_ids.extend(chunk_ids)\n",
    "        attention_masks.extend(chunk_attention_mask)\n",
    "        \n",
    "    return np.array(input_ids), np.array(attention_masks)\n",
    "\n",
    "\n",
    "def encode_input(sent1, sent2):\n",
    "    \"\"\"Encode the texts from two arrays of sentences\"\"\"\n",
    "    two_sent = np.char.add(np.char.add(sent1, \" [SEP] \"), sent2)\n",
    "    x = fast_encode(two_sent, fast_tokenizer, maxlen=MAX_LEN)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████| 1/1 [00:00<00:00, 656.90it/s]\n"
     ]
    }
   ],
   "source": [
    "sent1 = np.array([\"На каждой «руке» расположено до десяти тысяч вкусовых рецепторов, определяющих съедобность или несъедобность предмета.\", \n",
    "                  \"На каждой «руке» расположено до десяти тысяч вкусовых рецепторов, определяющих съедобность или несъедобность предмета.\"\n",
    "                ])\n",
    "sent2 = np.array([\"On each \"\"hand\"\" is up to ten thousand taste buds that determine edible or inedible object.\", \n",
    "                  \"The taste buds of the octopus is located on the hands, or rather on the suction cups.\"])\n",
    "y = np.array([1, 0])\n",
    "\n",
    "x = encode_input(sent1, sent2)\n",
    "\n",
    "sess = rt.InferenceSession(model_path, providers=rt.get_available_providers())\n",
    "\n",
    "input_name_0 = sess.get_inputs()[0].name\n",
    "input_name_1 = sess.get_inputs()[1].name\n",
    "\n",
    "input = {\n",
    "    input_name_0: np.array(x[0]).astype(np.int64),\n",
    "    input_name_1: np.array(x[1]).astype(np.int64)\n",
    "    }\n",
    "\n",
    "y_pred = sess.run(None, input)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.679295  ]\n",
      " [0.06985122]]\n",
      "[1 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0, 1.0, 1.0)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_pred)\n",
    "print(y)\n",
    "\n",
    "efficiency(y, y_pred, 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('clp': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b375303a199b0724c3d45bb934cae49d905d9d46fe283eb4c972c5b7dbad85fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
